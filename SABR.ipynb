{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68d4de48-57c3-455b-9668-92f022f498c3",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h1>SABR model machine learning</h1>\n",
    "<p style='width: 1000px;'>This notebook demonstrates an example of using machine learning with the SABR volatility model. First, we read the options market data provided. Second, fit the SABR model parameters for each ticker by minimization of the mean squared error. Finally, we fit the machine learning models and test their predictions.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "8246bd8e-96ea-4503-8989-020694c4247a",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/mnt/c/Users/Steve/implied_vol_machine_learning'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b9ab78-6971-4064-921f-404ba542757e",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>Read and process data</h2>\n",
    "<p style='width: 1000px;'>We read the option data and filter out puts, bad data, negative maturities (required for SABR), and deep out-of-the-money options. Since deep out-of-the-money options will have near-zero values, more datapoints in that range will not provide more value to the model, will require longer to fit parameters, and may sometimes skew the results. I'm not very confident it is right to remove them, or that 2.5 is a reasonable cutoff, so please raise this point in future discussions if you have more to offer.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f8a42c65-316f-4dd4-86ce-fecd89490264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quote dates: ['08/24/2022 16:00']\n",
      "Moneyness: min=0.005609846402405502, max=8750.0\n",
      "Maturity: min=-0.0027397260273972603, max=5.315068493150685\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "raw_data = pd.read_csv(os.path.join(root_dir, 'options_20220824.csv'))\n",
    "\n",
    "# For simplicity, let's only analyze call options\n",
    "# Also clean out some bad data\n",
    "call_data = raw_data.loc[(raw_data[\"Type\"] == \"call\") & (raw_data[\"Ask\"] < 99000.0)].copy()\n",
    "\n",
    "# Add some columns\n",
    "call_data.loc[:, \"moneyness\"] = call_data[\"Strike\"] / call_data[\"UnderlyingPrice\"]\n",
    "call_data.loc[:, \"implied_vol\"] = call_data[\"IV\"]\n",
    "call_data.loc[:, \"maturity\"] = (pd.to_datetime(call_data[\"Expiration\"]) - pd.to_datetime(call_data[\" DataDate\"])).dt.days / 365\n",
    "call_data.loc[:, \"ticker\"] = call_data[\"UnderlyingSymbol\"]\n",
    "call_data.loc[:, \"Mid\"] = (call_data[\"Bid\"]+call_data[\"Ask\"])/2\n",
    "print(f\"Quote dates: {call_data[' DataDate'].unique()}\")\n",
    "print(f\"Moneyness: min={call_data['moneyness'].min()}, max={call_data['moneyness'].max()}\")\n",
    "print(f\"Maturity: min={call_data['maturity'].min()}, max={call_data['maturity'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "182a0b6c-d4d6-4283-84fb-7e2c313a4cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the columns we don't need\n",
    "# Remove negative maturities and deep out of the money options\n",
    "model_input_data = call_data.loc[(call_data[\"maturity\"] > 0.0) & (call_data[\"moneyness\"] <= 2.5), [\"ticker\", \"moneyness\", \"maturity\", \"implied_vol\"]].copy() # we don't need other columns for this exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d44c3e6-58ba-4183-ba66-de4acf5f3043",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>Fit the model parameters</h2>\n",
    "<p style='width: 1000px;'>Using scipy.optimize.minimize and a mean-squared error function on QuantLib.sabrVolatility, we fit the model parameters &alpha; &isin; (0,&infin;), &beta; &isin; (0,1], &nu; &isin; (0, &infin;), and &rho; &isin; (-1,1) for every ticker. Since this fitting was time-consuming, the parameter values are saved to file model_parameters_by_ticker.csv.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18a9cd6-2b19-493e-9cf5-9fb7f58a09cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import QuantLib as ql\n",
    "import numpy as np\n",
    "\n",
    "# Create MSE function\n",
    "def sabr_vol_mse(model_params, maturity, moneyness, implied_vol):\n",
    "    alpha, beta, nu, rho = model_params\n",
    "    val = np.array([ql.sabrVolatility(float(money), 1, float(mat), alpha, beta, nu, rho) for mat, money in zip(maturity, moneyness)]) - implied_vol\n",
    "    return np.sqrt(val * val).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "21d40760-374d-4318-b27e-96ee040ceff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1194.0928888320923\n",
      "[]\n",
      "5685\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "alpha = 0.5; beta = 0.5; nu = 1.0; rho = 0.5\n",
    "start_values = [alpha, beta, nu, rho]\n",
    "bounds = [(0.0001, float('inf')), (0.0001, 1.0), (0.0001, float('inf')), (-0.9999, 0.9999)]\n",
    "\n",
    "model_params_by_ticker = {}\n",
    "error_tickers = []\n",
    "start = time.time()\n",
    "for ticker, ticker_data in model_input_data.groupby('ticker'):\n",
    "    res = minimize(sabr_vol_mse, start_values, bounds=bounds, args=(ticker_data['maturity'], ticker_data['moneyness'], ticker_data['implied_vol']), tol=1e-3, method=\"Powell\")\n",
    "    if res.success:\n",
    "        model_params_by_ticker[ticker] = res.x\n",
    "    else:\n",
    "        error_tickers.append(ticker)\n",
    "end = time.time()\n",
    "\n",
    "print(end-start)\n",
    "print(error_tickers)\n",
    "print(len(model_params_by_ticker))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d06aca35-f369-4aa9-891e-e357836098f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Alpha      Beta        Nu       Rho\n",
      "A     0.314901  0.651984  0.766485 -0.427394\n",
      "AA    0.617718  0.021392  0.474265  0.999899\n",
      "AADI  1.028776  0.373621  0.734218  0.999900\n"
     ]
    }
   ],
   "source": [
    "models = pd.DataFrame(model_params_by_ticker).T.rename(columns={0: 'Alpha', 1: 'Beta', 2: 'Nu', 3: 'Rho'})\n",
    "print(models.iloc[0:3])\n",
    "\n",
    "models.to_csv(os.path.join(root_dir, 'model_parameters_by_ticker.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de38b21-ea9a-41f9-92f3-8c65eb9d5fb4",
   "metadata": {},
   "source": [
    "<h2>Create volatilty surfaces</h2>\n",
    "<p style='width: 1000px;'>Using the solved-for SABR parameters and the model function QuantLib.sabrVolatility, generate a volatility surface for each ticker to use as our target data for training and testing the machine learning model. An alternative to using the model would be to use the empirical data as the target in the machine learning model, but aspects of the assignment suggested that I should use the theoretical model values. Since it is of interest to me, I also provide a section in which I use empirical data. Perhaps that was the intent of the assignment, anyway. In the case of the emprical data, I will not use the whole volatility surface as the target, but instead the individual points, turning moneyness and maturity into parameters, joining &alpha;, &beta;, &nu;, and &rho; for a total of six input parameters.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4677f90c-8921-4857-9bfc-ec9571ba1b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ticker     Alpha      Beta        Nu       Rho\n",
      "0      A  0.314901  0.651984  0.766485 -0.427394\n",
      "1     AA  0.617718  0.021392  0.474265  0.999899\n",
      "2   AADI  1.028776  0.373621  0.734218  0.999900\n",
      "Moneyness axis: [1.00000000e-03 2.78666667e-01 5.56333333e-01 8.34000000e-01\n",
      " 1.11166667e+00 1.38933333e+00 1.66700000e+00 1.94466667e+00\n",
      " 2.22233333e+00 2.50000000e+00]\n",
      "Maturity axis: [1.00000000e-03 5.56444444e-01 1.11188889e+00 1.66733333e+00\n",
      " 2.22277778e+00 2.77822222e+00 3.33366667e+00 3.88911111e+00\n",
      " 4.44455556e+00 5.00000000e+00]\n",
      "[[2.05734001 0.67870442 0.47705761 0.36024642 0.29476128 0.27800752\n",
      "  0.28766479 0.30419387 0.32123448 0.33720025]\n",
      " [4.17547152 0.76343738 0.66528544 0.63025282 0.61102312 0.59792982\n",
      "  0.58786711 0.57955642 0.57237815 0.56599448]]\n"
     ]
    }
   ],
   "source": [
    "models = pd.read_csv(os.path.join(root_dir, 'model_parameters_by_ticker.csv')).rename(columns={'Unnamed: 0' : 'ticker'})\n",
    "print(models.iloc[0:3])\n",
    "\n",
    "moneyness_axis = np.linspace(0.001, 2.5, 10)\n",
    "maturity_axis = np.linspace(0.001, 5.0, 10)\n",
    "print(f'Moneyness axis: {moneyness_axis}')\n",
    "print(f'Maturity axis: {maturity_axis}')\n",
    "vol_surfaces = np.zeros((models.shape[0], len(moneyness_axis)*len(maturity_axis)))\n",
    "\n",
    "for row_idx, row in models.iterrows():\n",
    "    vol_surfaces[row_idx,:] = np.array([ql.sabrVolatility(float(money), 1, float(mat), row['Alpha'], row['Beta'], row['Nu'], row['Rho']) for mat in maturity_axis for money in moneyness_axis])\n",
    "\n",
    "print(vol_surfaces[0:2,0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9ef5c250-4e53-4f41-a5a7-fdbfaa9672dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upper bounds: [3977.290955507308, 0.9999999999999925, 14835.60433962666, 0.9998999999981376]\n",
      "Lower bounds: [0.000100000000336, 0.0001000000000014, 0.0001000003836251, -0.9998999999931324]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    vol_surfaces, models.drop(columns=['ticker']).to_numpy(), test_size=0.15, random_state=42)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# scale = StandardScaler()\n",
    "scale2=  StandardScaler()\n",
    "# y_train_transform = scale.fit_transform(y_train)\n",
    "# y_test_transform = scale.transform(y_test)\n",
    "x_train_transform = scale2.fit_transform(X_train)\n",
    "x_test_transform = scale2.transform(X_test)\n",
    "\n",
    "# Get upper and lower bounds from model data\n",
    "# Rho probably didn't need transformation (already on (-1, 1)), but for uniformity of the code, I'll transform it\n",
    "ub=[models[column].max() for column in models.drop(columns='ticker').columns]\n",
    "lb=[models[column].min() for column in models.drop(columns='ticker').columns]\n",
    "print(f'Upper bounds: {ub}') # these honestly look a little crazy - might need to look into that later\n",
    "print(f'Lower bounds: {lb}') # likewise\n",
    "\n",
    "\n",
    "# Create transformation/scaling functions\n",
    "def myscale(x):\n",
    "    res=np.zeros(len(ub))\n",
    "    for i in range(len(ub)):\n",
    "        res[i]=(x[i] - (ub[i] + lb[i])*0.5) * 2 / (ub[i] - lb[i]) \n",
    "    return res\n",
    "\n",
    "\n",
    "def myinverse(x):\n",
    "    res=np.zeros(len(ub))\n",
    "    for i in range(len(ub)):\n",
    "        res[i]=x[i]*(ub[i] - lb[i]) *0.5 + (ub[i] + lb[i])*0.5\n",
    "    return res\n",
    "\n",
    "\n",
    "def xtransform(X_train,X_test):\n",
    "    return [scale2.transform(X_train),scale2.transform(X_test)] # this strikes me as a little strange (from a programming standpoint), but I can just do what they're doing for now\n",
    "\n",
    "\n",
    "def xinversetransform(x):\n",
    "    return scale2.inverse_transform(x)\n",
    "\n",
    "\n",
    "# Apply scaling/transformation to data\n",
    "[x_train_transform,x_test_transform]=xtransform(X_train,X_test)\n",
    "y_train_transform = np.array([myscale(y) for y in y_train])\n",
    "y_test_transform = np.array([myscale(y) for y in y_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f149d3-c4f8-4e39-a928-2a5df09299e2",
   "metadata": {},
   "source": [
    "<h2>Neural network on theoretical data</h2>\n",
    "<p style='width: 1000px;'>Using the methodology outlined by <a href='https://github.com/amuguruza/NN-StochVol-Calibrations/blob/master/Heston/NNHeston.ipynb'>Horvath, Muguruza, and Tomas</a>, re-scale the inputs to the keras neural network and fit the NN model, then test the predictions. Keras is used, instead of scikit, to keep in line with the paper's methodology. Additional areas of research for myself: more depth of knowledge and perhaps brute-force testing of different numbers of hidden layers in the neural network and other optimizers besides Adam (e.g. lbfgs or sgd). If this was intended to be a part of the exercise, I can revisit. I wouldn't say that there's a ton of value of trying to reproduce SABR's theoretical values as a neural network would, as QuantLib.sabrVolatility seems to perform quite well. It is unclear to me whether I have misinterpreted the intent of the exercise and I should have used empirical data, or if the exercise is indeed to simply show that I can set up and perform parameter fitting and a basic machine learning model. Regardless, the next section covers what it might look like to use empirical data as the target to the machine learning model.</p>\n",
    "<p style='width: 1000px;'>As a side note, I have left in some of the original author's sometimes strange and inconsequential coding choices, as I have made very few other changes to their methodology</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "4969c007-8dda-4b00-bdca-744c945fc830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_12 (InputLayer)       [(None, 4)]               0         \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 30)                150       \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 30)                930       \n",
      "                                                                 \n",
      " dense_46 (Dense)            (None, 30)                930       \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 100)               3100      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,110\n",
      "Trainable params: 5,110\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Time to fit: 61.4217267036438s\n"
     ]
    }
   ],
   "source": [
    "import keras, time\n",
    "from keras.layers import Activation\n",
    "from keras import backend as K\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "keras.backend.set_floatx('float64')\n",
    "\n",
    "input1 = keras.layers.Input(shape=(4,))\n",
    "x1 = keras.layers.Dense(30,activation = 'elu')(input1)\n",
    "x2=keras.layers.Dense(30,activation = 'elu')(x1) \n",
    "x3=keras.layers.Dense(30,activation = 'elu')(x2) \n",
    "x4=keras.layers.Dense(x_train_transform.shape[1],activation = 'linear')(x3)\n",
    "\n",
    "modelGEN = keras.models.Model(inputs=input1, outputs=x4)\n",
    "print(modelGEN.summary())\n",
    "\n",
    "start = time.time()\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "modelGEN.compile(loss = root_mean_squared_error, optimizer = \"adam\")\n",
    "modelGEN.fit(y_train_transform, x_train_transform, batch_size=32, validation_data = (y_test_transform, x_test_transform), epochs = 200, verbose = False, shuffle=1)#,callbacks=[earlystop])\n",
    "end = time.time()\n",
    "\n",
    "print(f'Time to fit: {end-start}s')\n",
    "\n",
    "modelGEN.save_weights(os.path.join(root_dir, 'sabr_kera_nn.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "6b0e9527-aa83-48d0-82af-b44a3d09b28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 0s 1ms/step\n",
      "Sum of errors: 0.0036128783456603896\n",
      "Average error: 4.235496302063763e-06\n"
     ]
    }
   ],
   "source": [
    "modelGEN.load_weights(os.path.join(root_dir, 'sabr_kera_nn.h5'))\n",
    "\n",
    "predictions = modelGEN.predict(y_test_transform)\n",
    "total_mse = root_mean_squared_error(predictions, x_test_transform)\n",
    "print(f'Sum of errors: {total_mse}')\n",
    "print(f'Average error: {total_mse/x_test_transform.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267cc8ce-1cea-4770-86a5-57586d9246ff",
   "metadata": {},
   "source": [
    "<h2>Machine learning on empirical data</h2>\n",
    "<p style='width: 1000px;'>Instead of using a theoretical volatility surface as the target, we fit the machine learning model to the empirical data provided. This may produce something that is only SABR-like as a model, but fit the empirical data more closely, almost as an independent model. In practice, this may provide much more value than a machine learning model on the theoretical values, but may not have been the intent of this exercise. Regardless, it was fun, so let's do it.</p>\n",
    "<p style='width: 1000px;'>For lack of experience and a better resource <a href='https://www.educative.io/blog/scikit-learn-cheat-sheet-classification-regression-methods'>this author</a> provides a starting place for some models I can try. Since my target in this case is continuous (i.e. a float), I need to stick to the regression models. Linear models can be ruled out in this case, due to the non-linear nature of the problem. (</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "82e90098-535f-4c3d-9feb-f13999557941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ticker  moneyness  maturity  implied_vol     Alpha      Beta        Nu  \\\n",
      "0      A   0.448867  0.060274       0.3615  0.314901  0.651984  0.766485   \n",
      "1      A   0.486272  0.060274       0.3615  0.314901  0.651984  0.766485   \n",
      "2      A   0.523678  0.060274       0.3615  0.314901  0.651984  0.766485   \n",
      "\n",
      "        Rho  \n",
      "0 -0.427394  \n",
      "1 -0.427394  \n",
      "2 -0.427394  \n"
     ]
    }
   ],
   "source": [
    "# Join the data into one dataframe, then split into X and y numpy arrays for training and testing\n",
    "models = pd.read_csv(os.path.join(root_dir, 'model_parameters_by_ticker.csv')).rename(columns={'Unnamed: 0' : 'ticker'})\n",
    "full_table = model_input_data.merge(models, left_on='ticker', right_on='ticker')\n",
    "print(full_table.iloc[0:3])\n",
    "\n",
    "y = full_table['implied_vol'].to_numpy()\n",
    "X = full_table[['Alpha', 'Beta', 'Nu', 'Rho', 'moneyness', 'maturity']].to_numpy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e588d70-e7b5-46bb-9b46-5762976a7a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruled out models\n",
    "\n",
    "# Linear - will not represent our model well\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "from sklearn.kernel_ridge import KernelRidge # huge memory requirement\n",
    "from sklearn.svm import SVR # took way too long to fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c49fc2-6f1a-4a71-8d9e-e69be103dd81",
   "metadata": {},
   "source": [
    "<h3>Neural network</h3>\n",
    "<p style='width: 1000px;'>Revisiting neural networks for empirical data, we should have pretty decent non-linear representation of our model, but at a cost to time. For lack of my own depth of knowledge, I merely copied the hidden layer sizes from the previous example. More experimentation could be done in that area. Values are scaled as recommended by scikit's documentation. I read something in the documentation about how partial_fit can provide live updates. That sounds like a nice feature, depending on the context.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "5fb94543-13f4-4a6f-bb88-29fea695689c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train: 185.36060881614685s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "\n",
    "scaler = StandardScaler()  \n",
    "scaler.fit(X_train)  \n",
    "NN_X_train = scaler.transform(X_train)  \n",
    "NN_X_test = scaler.transform(X_test)  \n",
    "\n",
    "start = time.time()\n",
    "regr = MLPRegressor(hidden_layer_sizes=(30, 30, 30), random_state=3, max_iter=500).fit(NN_X_train, y_train)\n",
    "end = time.time()\n",
    "print(f'Time to train: {end-start}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "6d5b532d-c0a7-4dcb-8a5a-392b6313831d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max diff: 17.255765892710517\n",
      "Mean diff: 0.11748781902633175\n",
      "Std of diffs: 0.27588991671591356\n"
     ]
    }
   ],
   "source": [
    "y_pred = regr.predict(NN_X_test)\n",
    "diffs = np.absolute(y_pred - y_test)\n",
    "print(f'Max diff: {diffs.max()}')\n",
    "print(f'Mean diff: {diffs.mean()}')\n",
    "print(f'Std of diffs: {diffs.std()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0829b1-784b-45fd-b162-33db068437a4",
   "metadata": {},
   "source": [
    "<h3>LightGBM histogram-based boosted decision trees</h3>\n",
    "<p style='width: 1000px;'>LightGBM's algorithm seemed to perform fairly well with the default parameters. Very, very fast and more accurate than the neural network.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "3344551f-fe07-4710-acfb-591b58aca8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train: 1.142179250717163s\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "start = time.time()\n",
    "lgbm_reg = LGBMRegressor(random_state=4).fit(X_train, y_train)\n",
    "end = time.time()\n",
    "print(f'Time to train: {end-start}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "a24b10e1-e331-4562-86c3-a235cd204308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max diff: 16.2218124340518\n",
      "Mean diff: 0.10211942758361592\n",
      "Std of diffs: 0.25252817156831264\n"
     ]
    }
   ],
   "source": [
    "y_pred = lgbm_reg.predict(X_test)\n",
    "diffs = np.absolute(y_pred - y_test)\n",
    "print(f'Max diff: {diffs.max()}')\n",
    "print(f'Mean diff: {diffs.mean()}')\n",
    "print(f'Std of diffs: {diffs.std()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73318858-bccf-41e8-890a-4eafdd6942c0",
   "metadata": {},
   "source": [
    "<h3>XG boosted decision trees</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "9538e0da-ec23-440e-b240-c204392c897c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train: 26.32429027557373s\n"
     ]
    }
   ],
   "source": [
    "from xgboost.sklearn import XGBRegressor\n",
    "\n",
    "start = time.time()\n",
    "xgboost_reg = XGBRegressor(random_state=5).fit(X_train, y_train)\n",
    "end = time.time()\n",
    "print(f'Time to train: {end-start}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "6d789968-54e7-4cf6-bc93-89fdab8ec76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max diff: 33.022717727422716\n",
      "Mean diff: 1.3003164203923936\n",
      "Std of diffs: 1.0461023824973916\n"
     ]
    }
   ],
   "source": [
    "y_pred = xgboost_reg.predict(NN_X_test)\n",
    "diffs = np.absolute(y_pred - y_test)\n",
    "print(f'Max diff: {diffs.max()}')\n",
    "print(f'Mean diff: {diffs.mean()}')\n",
    "print(f'Std of diffs: {diffs.std()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9cec27-8f8e-42df-bf6c-2a85b5ec81c7",
   "metadata": {},
   "source": [
    "<h3>CatBoost gradient boosted decistion tree</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "80d83a15-7bd3-42a4-b8e6-7dd0820c636f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train: 37.84508442878723s\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "\n",
    "start = time.time()\n",
    "catboost_reg = CatBoostRegressor(random_state=6, silent=True).fit(X_train, y_train)\n",
    "end = time.time()\n",
    "print(f'Time to train: {end-start}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "df18fd9d-e7d9-4da1-8dfa-9c0dcce49ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max diff: 31.125652441614317\n",
      "Mean diff: 0.5776743501730833\n",
      "Std of diffs: 0.5980525643315464\n"
     ]
    }
   ],
   "source": [
    "y_pred = catboost_reg.predict(NN_X_test)\n",
    "diffs = np.absolute(y_pred - y_test)\n",
    "print(f'Max diff: {diffs.max()}')\n",
    "print(f'Mean diff: {diffs.mean()}')\n",
    "print(f'Std of diffs: {diffs.std()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108846d0-e2af-427d-83e7-c9dcea22fb2c",
   "metadata": {},
   "source": [
    "<h3>Gradient boosting regression</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "38a0d12d-c11c-44dd-ada7-ea37c947c1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train: 88.6027729511261s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "start = time.time()\n",
    "gradboost_reg = GradientBoostingRegressor(random_state=7).fit(X_train, y_train)\n",
    "end = time.time()\n",
    "print(f'Time to train: {end-start}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "20ac4494-1b9a-40e8-9698-30493d6f27c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max diff: 32.49702334300319\n",
      "Mean diff: 0.556295996608095\n",
      "Std of diffs: 0.5504314877302316\n"
     ]
    }
   ],
   "source": [
    "y_pred = gradboost_reg.predict(NN_X_test)\n",
    "diffs = np.absolute(y_pred - y_test)\n",
    "print(f'Max diff: {diffs.max()}')\n",
    "print(f'Mean diff: {diffs.mean()}')\n",
    "print(f'Std of diffs: {diffs.std()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
